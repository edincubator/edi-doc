.. warning::

  Remember that for interacting with EDI Big Data Stack you must be
  authenticated at the system using `kinit` command. For more information, read
  the documentation at :ref:`authenticating-with-kerberos`.

.. _mapreduce:

MapReduce & YARN
================

EDI Big Data Stack provides the MapReduce implementation over YARN. We have
created a minimal example, based on
`Yelp dataset <https://www.kaggle.com/yelp-dataset/yelp-dataset/version/6>`_
that shows how to count how many Yelp businesses are in each USA state, and
how to submit this MapReduce to EDI Big Data Stack.

Yelp dataset is available for every user at `/samples/yelp`. Open a terminal at
Jyupyter Notebook and execute the following for inspecting the data:

.. code-block:: console

  # hdfs dfs -ls -h /samples/yelp
  Found 7 items
  drwx------   - hdfs hdfs          0 2018-10-10 15:05 /samples/yelp/yelp_business
  drwx------   - hdfs hdfs          0 2018-10-10 15:06 /samples/yelp/yelp_business_attributes
  drwx------   - hdfs hdfs          0 2018-10-10 15:06 /samples/yelp/yelp_business_hours
  drwx------   - hdfs hdfs          0 2018-10-10 15:06 /samples/yelp/yelp_checkin
  drwx------   - hdfs hdfs          0 2018-10-10 15:06 /samples/yelp/yelp_review
  drwx------   - hdfs hdfs          0 2018-10-10 15:07 /samples/yelp/yelp_tip
  drwx------   - hdfs hdfs          0 2018-10-10 15:07 /samples/yelp/yelp_user


You can find all examples at ~/work/examples directory at your Jupyter Lab
instance. A this section we will explain how to launch a typical map-reduce
work over Yelp dataset. You can find this example project at `mrexample`
folder. The relevant files at
this project are `BusinessPerStateCount.java` and `pom.xml`. Later, we are
going to inspect `BusinessPerStateCount.java` file.

The `BusinessPerStateCount.java` file contains the unique and main class of
this MapReduce job, the `BusinessPerStateCount` class, and two inner classes,
`RowTokenizerMapper` and `StateSumReducer`.

RowTokenizerMapper
..................

.. code-block:: java

  public static class RowTokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {

  private final static IntWritable one = new IntWritable(1);

  public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        // Extract state using opencsv library
        CSVReader reader = new CSVReader(new StringReader(value.toString()));
        String[] line;

        while ((line = reader.readNext()) != null) {
            // Check that current line is not CSV's header
            if (!line.equals("state")) {
                // Write "one" for current state to context
                context.write(new Text(line[5]), one);
            }
        }
    }
  }


The `RowTokenizerMapper` class represents the mapper of our job. Its definition
is very simple, as it only extends the base `Mapper` class, receiving a tuple
formed by a key of type `Object` and a value of type `Text` as input, and
generating a tuple formed by a key of type `Text` and a value of type
`IntWritable` as output.

The `map` method processes the input and generates the output that is passed
passed to the reducer. In this function, we take the value, representing the
state where the business is, and writes a tuple formed by the state as key, and
a "one" as a value. This allow us grouping all appearances of a state in the
reducer stage.


StateSumReducer
...............

.. code-block:: java

  public static class StateSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        // For each state coincidence, +one
        for (IntWritable val : values) {
            sum += val.get();
        }
        result.set(sum);

        // Return the state and the number of appearances.
        context.write(key, result);
    }
  }

The `StateSumReducer` class represents the reducer stage of our job. Similar to
the mapper, its definition states that it receives a tuple formed by key of type
`Text` and a value of type `IntWritable` (generated by the mapper) and produces
a tuple formed by key of type `Text` and a value of type `IntWritable`.

The `reduce` function executes the logic of the reducer stage. It receives a
key of type text and an `Iterable` of `IntWritables`. The MapReduce framework
groups all tuples generated at `RowTokenizerMapper` by its keys, and stores the
values for each key in a collection of `Iterable<IntWritable>` type. In the
case of our example, for each value in the `Iterable` collection, we iterate the
collection incrementing the counter obtaining the total count per key.

main
....

Finally, the `main` method of the `BusinessPerStateCount` class, which creates
and configures the job, has the following code:

.. code-block:: java

  public static void main(String [] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "state count");
    job.setJarByClass(BusinessPerStateCount.class);

    job.setMapperClass(RowTokenizerMapper.class);
    job.setReducerClass(StateSumReducer.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }

In the main method, the MapReduce job is configured. Concretely, this examples
sets the mapper and reducer classes, the output key and value classes and the
input and output directories (taken from the CLI when launching the job).

pom.xml
.......

The `pom.xml` file compiles the project and generates the jar that we need to
submit to EDI Big Data Stack.

.. code-block:: xml

  <?xml version="1.0" encoding="UTF-8"?>
  <project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>eu.edincubator.stack.examples</groupId>
    <artifactId>mr-example</artifactId>
    <version>1.0-SNAPSHOT</version>

    <build>
        <plugins>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <archive>
                        <manifest>
                            <mainClass>eu.edincubator.stack.examples.mr.BusinessPerStateCount</mainClass>
                        </manifest>
                    </archive>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
            </plugin>
        </plugins>
    </build>

    <dependencies>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-core</artifactId>
            <version>${hadoop.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>com.opencsv</groupId>
            <artifactId>opencsv</artifactId>
            <version>4.1</version>
        </dependency>
    </dependencies>

    <properties>
        <hadoop.version>2.7.3</hadoop.version>
    </properties>
  </project>


This file contains two important parts. The fist one, is the `<build>` block.
This block stablished how the jar is going to be built. In our case, we have
choose to create a "fat jar" including the third party dependencies
(`com.opencsv` library). On the other hand, the `<dependencies>` block contains
the dependencies of our project. It is important to import the correct version
of the libraries. For more information check :ref:`tools-and-versions`.

Compiling and submitting the job
................................

First, from a JupyterLab terminal, you must create the java package:

.. code-block:: console

  $ mvn clean compile assembly:single

Next, from the same terminal, we can submit the job with the `yarn jar`
command:

.. code-block:: console

  # cd target/
  # yarn jar mr-example-1.0-SNAPSHOT-jar-with-dependencies.jar /samples/yelp/yelp_business/yelp_business.csv /user/<username>/state-count-output
  18/10/10 08:03:49 INFO client.RMProxy: Connecting to ResourceManager at master.edincubator.eu/192.168.1.12:8050
  18/10/10 08:03:49 INFO client.AHSProxy: Connecting to Application History server at master.edincubator.eu/192.168.1.12:10200
  18/10/10 08:03:51 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 475 for <username> on 192.168.1.12:8020
  18/10/10 08:03:51 INFO security.TokenCache: Got dt for hdfs://master.edincubator.eu:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.1.12:8020, Ident: (HDFS_DELEGATION_TOKEN token 475 for <username>)
  18/10/10 08:03:53 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
  18/10/10 08:03:59 INFO input.FileInputFormat: Total input paths to process : 1
  18/10/10 08:04:02 INFO mapreduce.JobSubmitter: number of splits:1
  18/10/10 08:04:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1539081561867_0003
  18/10/10 08:04:04 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.1.12:8020, Ident: (HDFS_DELEGATION_TOKEN token 475 for <username>)
  18/10/10 08:04:06 INFO impl.TimelineClientImpl: Timeline service address: http://master.edincubator.eu:8188/ws/v1/timeline/
  18/10/10 08:04:09 INFO impl.YarnClientImpl: Submitted application application_1539081561867_0003
  18/10/10 08:04:09 INFO mapreduce.Job: The url to track the job: http://master.edincubator.eu:8088/proxy/application_1539081561867_0003/
  18/10/10 08:04:09 INFO mapreduce.Job: Running job: job_1539081561867_0003
  18/10/10 08:04:16 INFO mapreduce.Job: Job job_1539081561867_0003 running in uber mode : false
  18/10/10 08:04:16 INFO mapreduce.Job:  map 0% reduce 0%
  18/10/10 08:04:22 INFO mapreduce.Job:  map 100% reduce 0%
  18/10/10 08:04:30 INFO mapreduce.Job:  map 100% reduce 100%
  18/10/10 08:04:31 INFO mapreduce.Job: Job job_1539081561867_0003 completed successfully
  18/10/10 08:04:31 INFO mapreduce.Job: Counters: 49
  	File System Counters
  		FILE: Number of bytes read=1575775
  		FILE: Number of bytes written=3466869
  		FILE: Number of read operations=0
  		FILE: Number of large read operations=0
  		FILE: Number of write operations=0
  		HDFS: Number of bytes read=31760803
  		HDFS: Number of bytes written=425
  		HDFS: Number of read operations=6
  		HDFS: Number of large read operations=0
  		HDFS: Number of write operations=2
  	Job Counters
  		Launched map tasks=1
  		Launched reduce tasks=1
  		Data-local map tasks=1
  		Total time spent by all maps in occupied slots (ms)=165180
  		Total time spent by all reduces in occupied slots (ms)=130380
  		Total time spent by all map tasks (ms)=5506
  		Total time spent by all reduce tasks (ms)=4346
  		Total vcore-milliseconds taken by all map tasks=5506
  		Total vcore-milliseconds taken by all reduce tasks=4346
  		Total megabyte-milliseconds taken by all map tasks=169144320
  		Total megabyte-milliseconds taken by all reduce tasks=133509120
  	Map-Reduce Framework
  		Map input records=174568
  		Map output records=174568
  		Map output bytes=1226633
  		Map output materialized bytes=1575775
  		Input split bytes=129
  		Combine input records=0
  		Combine output records=0
  		Reduce input groups=69
  		Reduce shuffle bytes=1575775
  		Reduce input records=174568
  		Reduce output records=69
  		Spilled Records=349136
  		Shuffled Maps =1
  		Failed Shuffles=0
  		Merged Map outputs=1
  		GC time elapsed (ms)=684
  		CPU time spent (ms)=11800
  		Physical memory (bytes) snapshot=3349094400
  		Virtual memory (bytes) snapshot=57387188224
  		Total committed heap usage (bytes)=3968335872
  	Shuffle Errors
  		BAD_ID=0
  		CONNECTION=0
  		IO_ERROR=0
  		WRONG_LENGTH=0
  		WRONG_MAP=0
  		WRONG_REDUCE=0
  	File Input Format Counters
  		Bytes Read=31760674
  	File Output Format Counters
  		Bytes Written=425
  #

If the job is successfully executed, the result is written to the
`/user/<username>/state-count-output` directory. In case of any problem during
its execution, the error will be printed to the console. For further details
about the job, you can check the ResourceManager UI at
`<https://edi-master.novalocal:8443/gateway/hdp/yarnuiv2>`_.

TODO: update URL

Finally, if you check the output directory, you will see the result of the job
as a part-r-00000 file. The execution of this job generated a single file
because only one reducer is executed. However, the output could be split into
different files if more reducers were required to perform the job.

Then, we can list the files inside the output directory and print, directly to
the console, the contents of the generated file.
The `-cat` parameter shows the contents of the file, showing the number of
businesses for each USA state obtained as the result of the map reduce job.

.. code-block:: console

  # hdfs dfs -ls /user/<username>/state-count-output
  Found 2 items
  -rw-------   3 <username> <username>          0 2018-04-13 08:11 /user/<username>/state-count-output/_SUCCESS
  -rw-------   3 <username> <username>        425 2018-04-13 08:11 /user/<username>/state-count-output/part-r-00000
  # hdfs dfs -cat /user/<username>/state-count-output/part-r-00000
  1
  01	10
  3	1
  30	1
  6	3
  AB	1
  ABE	3
  AK	1
  AL	1
  AR	2
  AZ	52214
  B	1
  BW	3118
  BY	4
  C	28
  CA	5
  CHE	143
  CMA	2
  CO	2
  CS	1
  DE	1
  EDH	3795
  ELN	47
  ESX	12
  FAL	1
  FIF	85
  FL	1
  FLN	2
  GA	1
  GLG	3
  HLD	179
  HU	1
  IL	1852
  IN	3
  KHL	1
  KY	1
  MLN	208
  MN	1
  MT	1
  NC	12956
  NE	1
  NI	10
  NLK	1
  NTH	2
  NV	33086
  NY	18
  NYK	152
  OH	12609
  ON	30208
  PA	10109
  PKN	1
  QC	8169
  RCC	1
  SC	679
  SCB	5
  SL	1
  ST	11
  STG	1
  TAM	1
  VA	1
  VS	7
  VT	2
  WA	1
  WHT	1
  WI	4754
  WLN	38
  XGL	4
  ZET	1
  #
