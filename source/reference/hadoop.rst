.. _hadoop_reference:

Hadoop - MapReduce
===================

This section compiles references to external resources that contain further
information about the MapReduce programming model and the Hadoop framework.

The goal of this section is to provide a general knowledge about the involved
paradigms and technologies, which can be used to improve the implementation of
the different solutions. However, this section is not intended to be a detailed
manual of all the involved aspects.

The official page of the `Apache Hadoop <http://hadoop.apache.org/>`_ project is
good starting point to learn about the technology, API and related tools.


Architecture overview
---------------------

The following resource provides a general view of the internal
`architecture <http://ercoppa.github.io/HadoopInternals/HadoopArchitectureOverview.html>`_
of the Hadoop framework.
Although all this processes are transparently managed by Hadoop framework, it is
important to understand the relation between the different components in order
to develop solutions that behave efficiently and use all the resources in a
correct manner.


MapReduce process
-----------------

A general MapReduce job involves different phases that must be understood by
developers to create scalable processes and make a correct use
of the distributed computation environment. The following
`Wikipedia entry <https://en.wikipedia.org/wiki/MapReduce>`_ provides a good
tarting poing to understand the general process of a MapReduce job.

A MapReduce job is composed of the following phases:

 Input reading -> Mapper -> Shuffling -> Reducer -> Output writing


Input reading
~~~~~~~~~~~~~

In addition to the different formats that the Hadoop framework supports for data
I/O, it is necessary to have in mind two important concepts when implementing
solutions in the Hadoop framework: data locality and data splitability.

First, data locality refers to how the data is retrieved from the distributed
storage system. Hadoop works on a local-first policy, obtaining data, if possible,
from the blocks located in the same node that is going to process them. This
approach tries to minimize the network transmission between different computation
nodes. However, this is not always possible depending of the data was distributed
by HDFS. The following link provides more information about the
`data locality <https://netjs.blogspot.com/2018/04/data-locality-in-hadoop.html>`_
in Hadoop.

On the other hand, there is a process known as InputSplit in Hadoop, which refers
to how the data is logically split in order to be processed by the different
mappers of a job. For example, plain text files can be split into multiple
chunks of lines that could be processed independently by different map processes.
The split process and its relation with the data locality is explained by the
following `link <https://data-flair.training/blogs/mapreduce-inputsplit-vs-block-hadoop/>`_.

However, it is important to remember that not all the input data formats are
splitable. For example, some compressed formats (e.g. gzip) cannot be directly
split into multiple nodes, meaning that the whole file must be retrieved and
decompressed by a single node before the data can be distributed and processed
by the cluster. Therefore, it is important to use, as input data for processing,
formats which can be split and distributed among the nodes.

Mapper
~~~~~~

The map is the function that takes one element of data, i.e. a pair <K1, V1>,
performs the required transformations, and returns a list of <K2, V2>, which can
be in a different domain from the input.

Combiner
~~~~~~~~

The map function is applied in parallel to each <K1, V1> from the input, however,
the results of different executions of the mapper can be combined before sending
them to the reducers in order to minimize the data transmission across the
network. It is important to note that the combiner function can be applying
more than one before sending the data from the mappers.
More information about the combiner process and how it can improve the
MapReduce jobs can be obtained in the following
`tutorial <https://data-flair.training/blogs/hadoop-combiner-tutorial/>`_.

Shuffling
~~~~~~~~~

After the mappers the data, and it is further combined if possible, the
data must be sent to the reducer processes. The process of deciding which data
goes to each available reducer is controlled by this step, particularly, the
partitioning and the sorting.

The partitioner decides which keys are sent to each available reducer. The
following tutorial contains more information about the
usage of a `custom partitioner <https://data-flair.training/blogs/hadoop-partitioner-tutorial/>`_.

On the other hand, the sorting process is applied to the keys generated by the
mappers to compare them before being sent to the reducers. It is possible to
implement custom comparators to obtain specific behaviours during the MapReduce
process. In addition, there is also possible to implement a seconday sorting when
the data is received by the reducers. The utility of the sorting phase is
explained `here <https://data-flair.training/blogs/shuffling-and-sorting-in-hadoop/>`_.

Reducer
~~~~~~~

The reduce phase obtains the list of pairs from the different mappers and group
them to apply a process that obtains the output data in a different domain,
meaning that it receives a pairs of <K2, list(V2)>, after the shuffling, and
outputs a list of <K3, V3>.

Output writing
~~~~~~~~~~~~~~

Hadoop supports, out-of-the-box, multiple output data formats, however, it can
also extended to support custom formats if needed. The following tutorial
provides a summary of some of the supported
`data formats <https://community.hitachivantara.com/community/products-and-solutions/pentaho/blog/2017/11/07/hadoop-file-formats-its-not-just-csv-anymore>`_
(also applicable to input data reading).


Development process
-------------------

This section includes some comments and advices about how to improve the
development of jobs withing the Hadoop framework.

Building
~~~~~~~~

It is recommendable to build Hadoop applications using tools such as
`Apache Maven <https://maven.apache.org/>`_ to manage the building dependencies.
This `repository <https://github.com/meniluca/maven-hadoop-java-wordcount-template>`_
contains an template for creating projects with this building tool. In addition,
this documentation contains also a full example of an application
building, see :ref:`mapreduce`.

Testing
~~~~~~~

As any other software development, developed code should be tested before
launching the job with real data.

First, Hadoop provides libraries to facilitate the testing of the developed code
before the execution with real data. There are tools such as MRUnit, which allow
to test the different components of the MapReduce job: driver program, map
function, reduce function, combiners, etc. The following link contains an
introduction tutorial to the usage of `MRUnit <https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial>`_.


Debugging
~~~~~~~~~

On the other hand, jobs must be tested with sample data using the
`standalone mode <https://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation>`_
before launching the a real job in the cluster. Testing the jobs thoroughly
reduces the development time and the detection of errors in production.
