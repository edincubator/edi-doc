.. _mapreduce:

MapReduce (YARN)
=================

EDI Big Data Stack provides the MapReduce implementation over YARN. Following the
example started at :ref:`hdfs`, we have created a simple example that shows how
to count how many Yelp businesses are in each USA state, and how to submit
this MapReduce to EDI Big Data Stack.

Fist, you must clone the repository containing examples and move into
`mrexample` dir.

.. code-block:: console

  $ git clone https://github.com/edincubator/stack-examples
  $ cd stack-examples/mrexample

The relevant files at this project are `BusinessPerStateCount.java` and
`pom.xml`. Next we are going to inspect `BusinessPerStateCount.java` file.

`BusinessPerStateCount.java` file contains the unique and main class of this
MapReduce job. `BusinessPerStateCount` class contains two subclasses:
`RowTokenizerMapper` and `StateSumReducer`.

RowTokenizerMapper
------------------

.. code-block:: java

  public static class RowTokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {

  private final static IntWritable one = new IntWritable(1);

  public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        // Extract state using opencsv library
        CSVReader reader = new CSVReader(new StringReader(value.toString()));
        String[] line;

        while ((line = reader.readNext()) != null) {
            // Check that current line is not CSV's header
            if (!line.equals("state")) {
                // Write "one" for current state to context
                context.write(new Text(line[5]), one);
            }
        }
    }
  }


The `RowTokenizerMapper` class represents the mapper of our job. Its definition
is very simple. It extends the `Mapper` class, receiving a tuple formed by a
key of type `Object` and a value of type `Text` as input, and generating a tuple
formed by a key of type `Text` and a value of type `IntWritable` as output.

The `map` method is who processes the input and generates the output to be
passed to the reducer. In this function, we take the value, representing the
state where the business is, and writes a tuple formed by the state as key, and
a "one" as a value. This allow us grouping all appearances of a state in the
reducer stage.


StateSumReducer
---------------

.. code-block:: java

  public static class StateSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        // For each state coincidence, +one
        for (IntWritable val : values) {
            sum += val.get();
        }
        result.set(sum);

        // Return the state and the number of appearances.
        context.write(key, result);
    }
  }

The `StateSumReducer` class represents the reducer stage of our job. Similar to
the mapper, its definition states that receives a tuple formed by key of type
`Text` and a value of type `IntWritable` (generated by the mapper) and produces
a tuple formed by key of type `Text` and a value of type `IntWritable`.

The `reduce` function executes the logic of the reducer stage. It receives a key
of type text and an `Iterable` of `IntWritables`. The MapReduce framework groups
all tuples generated at `RowTokenizerMapper` by its keys, and groups the values
for each keys in a collection of `Iterable<IntWritable>` type. This way, for
each "one" value in the `Iterable` we can improve the result by one and know
how many business a state has.

main
----

At last, check the `main` method of the `BusinessPerStateCount` class.

.. code-block:: java

  public static void main(String [] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "state count");
    job.setJarByClass(BusinessPerStateCount.class);

    job.setMapperClass(RowTokenizerMapper.class);
    job.setReducerClass(StateSumReducer.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }

In the main method, the MapReduce job is configured. Concretely, in this example
mapper and reducer classes, output key and value classes and input and output
directories (taken from the CLI when launching the job) are set.

pom.xml
-------

The `pom.xml` file compiles the project and generates the jar that we need to
submit to EDI Big Data Stack.

.. code-block:: xml

  <?xml version="1.0" encoding="UTF-8"?>
  <project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>eu.edincubator.stack.examples</groupId>
    <artifactId>mr-example</artifactId>
    <version>1.0-SNAPSHOT</version>

    <build>
        <plugins>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <archive>
                        <manifest>
                            <mainClass>eu.edincubator.stack.examples.mr.BusinessPerStateCount</mainClass>
                        </manifest>
                    </archive>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
            </plugin>
        </plugins>
    </build>

    <dependencies>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-core</artifactId>
            <version>${hadoop.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>com.opencsv</groupId>
            <artifactId>opencsv</artifactId>
            <version>4.1</version>
        </dependency>
    </dependencies>

    <properties>
        <hadoop.version>2.7.3</hadoop.version>
    </properties>
  </project>


This file contains two important parts. The fist one, is the `<build>` block.
This block stablished how the jar is going to be built. In our case, we have
choose to create a "fat jar" including the third party dependencies
(`com.opencsv` library). On the other hand, the `<dependencies>` block contains
the dependencies of our project. It is important to import the correct version
of the libraries. For more information check :ref:`tools-and-versions`.

Compiling and submitting the job
--------------------------------

At first, you must create the java package and copy it to your workspace:

.. code-block:: console

  $ mvn clean compile assembly:single
  $ cp target/mr-example-1.0-SNAPSHOT-jar-with-dependencies.jar <workdir>

Next, at stack-client docker cointainer, we can submit the job using the
`hadoop jar` command:

.. note::

  We recommend deploying a local hadoop instance in your own machine for testing
  your projects before submitting them to EDI Big Data Stack. Read
  :ref:`deploying-local-hadoop` for more information about how to do this.

.. warning::

  Remember that for interacting with EDI Big Data Stack you must be
  authenticated at the system using `kinit` command. For more information, read
  the documentation at :ref:`authenticating-with-kerberos`.

.. code-block:: console

  # cd /workdir
  # hadoop jar mr-example-1.0-SNAPSHOT-jar-with-dependencies.jar /user/mikel/samples/yelp_business.csv /user/mikel/state-count-output
  18/04/13 08:10:38 INFO client.RMProxy: Connecting to ResourceManager at gauss.res.eng.it/192.168.125.113:8050
  18/04/13 08:10:38 INFO client.AHSProxy: Connecting to Application History server at gauss.res.eng.it/192.168.125.113:10200
  18/04/13 08:10:38 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 532 for mikel on 192.168.125.113:8020
  18/04/13 08:10:39 INFO security.TokenCache: Got dt for hdfs://gauss.res.eng.it:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.125.113:8020, Ident: (HDFS_DELEGATION_TOKEN token 532 for mikel)
  18/04/13 08:10:39 INFO security.TokenCache: Got dt for hdfs://gauss.res.eng.it:8020; Kind: kms-dt, Service: 192.168.125.113:9292, Ident: (owner=mikel, renewer=yarn, realUser=, issueDate=1523607038981, maxDate=1524211838981, sequenceNumber=204, masterKeyId=50)
  18/04/13 08:10:39 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
  18/04/13 08:10:39 INFO input.FileInputFormat: Total input paths to process : 1
  18/04/13 08:10:39 INFO mapreduce.JobSubmitter: number of splits:1
  18/04/13 08:10:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523347765873_0008
  18/04/13 08:10:40 INFO mapreduce.JobSubmitter: Kind: kms-dt, Service: 192.168.125.113:9292, Ident: (owner=mikel, renewer=yarn, realUser=, issueDate=1523607038981, maxDate=1524211838981, sequenceNumber=204, masterKeyId=50)
  18/04/13 08:10:40 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: 192.168.125.113:8020, Ident: (HDFS_DELEGATION_TOKEN token 532 for mikel)
  18/04/13 08:10:41 INFO impl.TimelineClientImpl: Timeline service address: http://gauss.res.eng.it:8188/ws/v1/timeline/
  18/04/13 08:10:42 INFO impl.YarnClientImpl: Submitted application application_1523347765873_0008
  18/04/13 08:10:42 INFO mapreduce.Job: The url to track the job: http://gauss.res.eng.it:8088/proxy/application_1523347765873_0008/
  18/04/13 08:10:42 INFO mapreduce.Job: Running job: job_1523347765873_0008
  18/04/13 08:11:02 INFO mapreduce.Job: Job job_1523347765873_0008 running in uber mode : false
  18/04/13 08:11:02 INFO mapreduce.Job:  map 0% reduce 0%
  18/04/13 08:11:24 INFO mapreduce.Job:  map 100% reduce 0%
  18/04/13 08:11:31 INFO mapreduce.Job:  map 100% reduce 100%
  18/04/13 08:11:31 INFO mapreduce.Job: Job job_1523347765873_0008 completed successfully
  18/04/13 08:11:32 INFO mapreduce.Job: Counters: 49
  	File System Counters
  		FILE: Number of bytes read=1575775
  		FILE: Number of bytes written=3468115
  		FILE: Number of read operations=0
  		FILE: Number of large read operations=0
  		FILE: Number of write operations=0
  		HDFS: Number of bytes read=31760804
  		HDFS: Number of bytes written=425
  		HDFS: Number of read operations=6
  		HDFS: Number of large read operations=0
  		HDFS: Number of write operations=2
  	Job Counters
  		Launched map tasks=1
  		Launched reduce tasks=1
  		Data-local map tasks=1
  		Total time spent by all maps in occupied slots (ms)=40282
  		Total time spent by all reduces in occupied slots (ms)=8552
  		Total time spent by all map tasks (ms)=20141
  		Total time spent by all reduce tasks (ms)=4276
  		Total vcore-milliseconds taken by all map tasks=20141
  		Total vcore-milliseconds taken by all reduce tasks=4276
  		Total megabyte-milliseconds taken by all map tasks=30936576
  		Total megabyte-milliseconds taken by all reduce tasks=8757248
  	Map-Reduce Framework
  		Map input records=174568
  		Map output records=174568
  		Map output bytes=1226633
  		Map output materialized bytes=1575775
  		Input split bytes=130
  		Combine input records=0
  		Combine output records=0
  		Reduce input groups=69
  		Reduce shuffle bytes=1575775
  		Reduce input records=174568
  		Reduce output records=69
  		Spilled Records=349136
  		Shuffled Maps =1
  		Failed Shuffles=0
  		Merged Map outputs=1
  		GC time elapsed (ms)=1325
  		CPU time spent (ms)=16090
  		Physical memory (bytes) snapshot=1443233792
  		Virtual memory (bytes) snapshot=6987718656
  		Total committed heap usage (bytes)=1478492160
  	Shuffle Errors
  		BAD_ID=0
  		CONNECTION=0
  		IO_ERROR=0
  		WRONG_LENGTH=0
  		WRONG_MAP=0
  		WRONG_REDUCE=0
  	File Input Format Counters
  		Bytes Read=31760674
  	File Output Format Counters
  		Bytes Written=425
  #

The job is executed successfully and the result put at
`/user/mikel/state-count-output` directory. In case of any error, it will be
shown at console. For further details about the job, you can check the
ResourceManager UI at `<http://RESOURCEMANAGERURL:8088/cluster>`_.

.. todo::

  Replace RESOURCEMANAGERURL by production URL.

At last, if you check the output directory, you will see the result of the job
at part-r-00000 file. The execution of this job generated a single file as a
single reducer is executed.

.. code-block:: console

  # hdfs dfs -ls /user/mikel/state-count-output
  Found 2 items
  -rw-------   3 mikel mikel          0 2018-04-13 08:11 /user/mikel/state-count-output/_SUCCESS
  -rw-------   3 mikel mikel        425 2018-04-13 08:11 /user/mikel/state-count-output/part-r-00000
  # hdfs dfs -cat /user/mikel/state-count-output/part-r-00000
  1
  01	10
  3	1
  30	1
  6	3
  AB	1
  ABE	3
  AK	1
  AL	1
  AR	2
  AZ	52214
  B	1
  BW	3118
  BY	4
  C	28
  CA	5
  CHE	143
  CMA	2
  CO	2
  CS	1
  DE	1
  EDH	3795
  ELN	47
  ESX	12
  FAL	1
  FIF	85
  FL	1
  FLN	2
  GA	1
  GLG	3
  HLD	179
  HU	1
  IL	1852
  IN	3
  KHL	1
  KY	1
  MLN	208
  MN	1
  MT	1
  NC	12956
  NE	1
  NI	10
  NLK	1
  NTH	2
  NV	33086
  NY	18
  NYK	152
  OH	12609
  ON	30208
  PA	10109
  PKN	1
  QC	8169
  RCC	1
  SC	679
  SCB	5
  SL	1
  ST	11
  STG	1
  TAM	1
  VA	1
  VS	7
  VT	2
  WA	1
  WHT	1
  WI	4754
  WLN	38
  XGL	4
  ZET	1
  #

`-cat` parameter shows the contents of the file, showing the number of
businesses for each USA state, although some cleaning is still need.
